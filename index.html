<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space">
  <meta property="og:title" content="REVS: Unlearning Sensitive Information in Language Models"/>
  <meta property="og:description" content="REVS enables language models to 'unlearn' sensitive or private information from their training data, while maintaining performance on general tasks and resisting adversarial extraction attempts."/>
  <meta property="og:url" content="https://tomertech.github.io/REVS-Web/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200x630-->
  <meta property="og:image" content="static/image/REVS banner.png" />
  <meta property="og:image:width" content="600"/>
  <meta property="og:image:height" content="200"/>


  <meta name="twitter:title" content="REVS: Unlearning Sensitive Information in Language Models">
  <meta name="twitter:description" content="REVS surgically removes a language model's tendency to generate a given sensitive information from its training data, while preserving its broader knowledge.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200x600-->
  <meta name="twitter:image" content="static/images/REVS banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="language models, unlearning, model editing, privacy, sensitive information">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>REVS: Unlearning Sensitive Information in Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <!-- Section with light orange background for paper title -->
  <section class="hero publication-title-section">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/tomerashuach/" target="_blank">Tomer Ashuach</a>,</span>
                <span class="author-block">
                  <a href="https://mttk.github.io/" target="_blank">Martin Tutek</a>,</span>
                  <span class="author-block">
                    <a href="https://belinkov.com/" target="_blank">Yonatan Belinkov</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Technion â€“ Israel Institute of Technology</span> 
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2406.09325v2" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Tomertech/REVS" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.09325v2" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Language models (LMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset scrubbing, or model filtering through unlearning and model editing, which can be bypassed through extraction attacks. We propose REVS, a novel non-gradient-based method for unlearning sensitive information from LMs. REVS identifies and modifies a small subset of neurons relevant for constituent tokens which form sensitive information. To adequately evaluate our method on truly sensitive information, we curate two datasets: an email dataset naturally memorized by Llama-3-8B and GPT-J-6B, and a synthetic social security number dataset that we tune the models to memorize. Compared to other methods, REVS demonstrates superior performance in unlearning sensitive information and robustness to extraction attacks, while retaining underlying model integrity.          </p>
        </div>
      </div>
    </div>
  </div>
<!-- End paper abstract -->

<!-- Motivation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>Language models risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in their training data, causing privacy concerns. Current approaches to address this issue involve costly dataset scrubbing or model filtering through unlearning and model editing, which can be bypassed through extraction attacks.</p>
          <figure class="wide-figure">
            <img src="static/images/High Level Main Method Plot Wide tinypng.png" alt="Figure 1 from the paper">
            <figcaption>Overview of the REVS unlearning process: (1) The original model memorizes a sensitive email address and (2) generates it exactly given a related prompt using greedy decoding. (3) After applying REVS, the target email token(s) are demoted to a specified lower rank <var>R</var> in the model's output, preventing the model from generating the unlearned email.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>REVS selects tokens from the target sensitive information sequence, as accurately extracting the original sensitive data requires recovering the full token sequence. It then locates the relevant layers for those tokens and selects neurons within those layers that are both relevant to the given prompt and have high association with the target token information. REVS iteratively edits these selected neurons to <b>reduce the rank</b> of the target tokens below a specified threshold, effectively <i>demoting</i> the sensitive information while preserving the model's broader knowledge.</p>
          <figure class="wide-figure">
            <img src="static/images/Main Method Plot Wide tinypng.png" alt="Figure 2 from the paper">
            <figcaption>Editing one neuron with REVS: (1) The neuron is projected from hidden space to vocabulary logit space. (2) The logit is adjusted to demote the target token rank to a desired lower rank <var>R</var>. (3) The adjusted logits vector is projected back to hidden space, yielding the updated neuron value.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Locating Model Components for Editing -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Locating Model Components for Editing</h2>
        <div class="content has-text-justified">
          <p>Our methodology REVS selects the <b>rarest</b> <i>tokens</i> from the target sensitive information sequence, identifies <i>layers</i> where those tokens have <b>high rank</b> in the residual hidden state, and selects <i>neurons</i> within those layers that exhibit both <b>high activations for the given prompt and high rank for the target tokens</b> when projected to the vocabulary space. This targeted approach aims to surgically remove the model's tendency to generate the sensitive information while preserving its broader knowledge.</p>
        </div>
      </div>
    </div>
  </div>
</section>

</section>
<!-- Results -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          REVS achieves near-perfect efficacy in preventing the generation of unlearned targets and maintains high specificity, indicating minimal disruption to the model's desired behavior. 
          Notably, our experiments revealed inherent differences in unlearning performance between synthetic and organically memorized information, with the organically memorized Email dataset proving more challenging, suggesting that information memorized during pre-training may be more deeply ingrained in the model's parameters.
        </div>
      </div>
    </div>
  </div>

  <style>
    table {
        width: 80%;
        max-width: 800px; /* Set the maximum width of the tables */
        border-collapse: collapse;
        margin-bottom: 20px;
        margin-top: 20px;
        margin-left: auto;
        margin-right: auto;
        background-color: #f2f2f2;
    }
    th, td {
        border: 2px solid #a29999;
        text-align: center;
        padding: 8px;
        font-size: 0.8em;
        background-color: #ffffff;
    }
    th {
        background-color: #f2f2f2;
    }
    caption {
        padding: 8px;
        font-weight: bold;
        text-align: center;
        font-size: 1.0em;
    }
</style>

  <table>
    <caption>Unlearning effectiveness and model integrity results. REVS is superior in almost all cases.</caption>
    <tr>
      <th rowspan="2">Dataset</th>
      <th rowspan="2">Method</th>
      <th rowspan="2">Harmonic Mean &uparrow;</th>
      <th rowspan="2">Efficacy@100 &uparrow;</th>
      <th rowspan="2">General.@100 &uparrow;</th>
      <th rowspan="2">Specificity &uparrow;</th>
      <th rowspan="2">Perplexity &downarrow;</th>
    </tr>
    <tr></tr>
    <tr>
      <td rowspan="4">SSN</td>
      <td>Unedited</td>
      <td>0&plusmn;0.0</td>
      <td>0&plusmn;0.0</td>
      <td>0&plusmn;0.0</td>
      <td><u>100&plusmn;0</u></td>
      <td><u>11.148&plusmn;0</u></td>
    </tr>
    <tr>
      <td>FT-L</td>
      <td>4.78&plusmn;3.66</td>
      <td>55.78&plusmn;12.37</td>
      <td>1.75&plusmn;1.44</td>
      <td>61.67&plusmn;15.17</td>
      <td>11.27&plusmn;0.054</td>
    </tr>
    <tr>
      <td>MEMIT (modified)</td>
      <td>78.07&plusmn;2.2</td>
      <td>98.5&plusmn;2.33</td>
      <td>61.15&plusmn;3.25</td>
      <td><strong>84.17&plusmn;3.07</strong></td>
      <td>11.156&plusmn;0.011</td>
    </tr>
    <tr>
      <td><strong>REVS (ours)</strong></td>
      <td><strong>81.45&plusmn;3.56</strong></td>
      <td><strong>99.95&plusmn;0.07</strong></td>
      <td><strong>80.17&plusmn;3.22</strong></td>
      <td>70.33&plusmn;7.84</td>
      <td>11.165&plusmn;0.01</td>
  </tr>
    <tr>
      <td rowspan="4">Emails</td>
      <td>Unedited</td>
      <td>0&plusmn;0.0</td>
      <td>0&plusmn;0.0</td>
      <td>-</td>
      <td><u>100&plusmn;0</u></td>
      <td><u>8.129&plusmn;0</u></td>
    </tr>
    <tr>
      <td>FT-L</td>
      <td>3.33&plusmn;1.77</td>
      <td>55.57&plusmn;4.39</td>
      <td>-</td>
      <td>1.73&plusmn;0.96</td>
      <td>13.63&plusmn;0.34</td>
    </tr>
    <tr>
      <td>MEMIT (modified)</td>
      <td>70.05&plusmn;1.16</td>
      <td>88.23&plusmn;1.64</td>
      <td>-</td>
      <td>58.1&plusmn;1.63</td>
      <td>8.13&plusmn;0</td>
    </tr>
    <tr>
      <td><strong>REVS (ours)</strong></td>
      <td><strong>80.65&plusmn;2.41</strong></td>
      <td><strong>97.22&plusmn;1.04</strong></td>
      <td><strong>-</strong></td>
      <td><strong>68.98&plusmn;3.6</strong></td>
      <td>8.148&plusmn;0.002</td>
  </tr>
  </table>
  
  <table>
    <caption>Average results for extraction resistance. REVS is more robust to extraction attacks.</caption>
    <tr>
      <th rowspan="2">Dataset</th>
      <th rowspan="2">Method</th>
      <th rowspan="2">Harmonic Mean &uparrow;</th>
      <th rowspan="2">Logit Lens@100 &uparrow;</th>
      <th rowspan="2">Delta@100 &uparrow;</th>
      <th rowspan="2">Perturb@100 &uparrow;</th>
    </tr>
    <tr></tr>
    <tr>
      <td rowspan="4">SSN</td>
      <td>Unedited</td>
      <td>0&plusmn;0.0</td>
      <td>0&plusmn;0.0</td>
      <td>95.12&plusmn;0.82</td>
      <td>26.5&plusmn;5.26</td>
    </tr>
    <tr>
      <td>FT-L</td>
      <td>65.35&plusmn;7.57</td>
      <td>55.63&plusmn;12.64</td>
      <td>97.03&plusmn;0.8</td>
      <td>58.62&plusmn;3.49</td>
    </tr>
    <tr>
      <td>MEMIT (modified)</td>
      <td>93.52&plusmn;1.76</td>
      <td>97.48&plusmn;2.01</td>
      <td>97.88&plusmn;0.64</td>
      <td>90.93&plusmn;3.53</td>
    </tr>
    <tr>
      <td><strong>REVS (ours)</strong></td>
      <td><strong>99.12&plusmn;3.56</strong></td>
      <td><strong>99.95&plusmn;0.07</strong></td>
      <td><strong>98.55&plusmn;0.2</strong></td>
      <td><strong>98.97&plusmn;1.46</strong></td>
  </tr>
    <tr>
      <td rowspan="4">Emails</td>
      <td>Unedited</td>
      <td>0&plusmn;0.0</td>
      <td>0&plusmn;0.0</td>
      <td>83.8&plusmn;0.67</td>
      <td>44.2&plusmn;4.11</td>
    </tr>
    <tr>
      <td>FT-L</td>
      <td>50.15&plusmn;3.08</td>
      <td>28.47&plusmn;2.73</td>
      <td>85.83&plusmn;1.11</td>
      <td>78.08&plusmn;5.15</td>
    </tr>
    <tr>
      <td>MEMIT (modified)</td>
      <td>80.73&plusmn;1.7</td>
      <td>79.62&plusmn;2.31</td>
      <td>86.17&plusmn;0.39</td>
      <td>77.12&plusmn;3.86</td>
    </tr>
    <tr>
      <td><strong>REVS (ours)</strong></td>
      <td><strong>83.48&plusmn;1.14</strong></td>
      <td><strong>81.05&plusmn;1.17</strong></td>
      <td><strong>87.08&plusmn;0.25</strong></td>
      <td><strong>82.63Â±2.63</strong></td>
  </tr>
  </table>
</section>


</body>
</html>
</section>
<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{tomer2024revs,
        title={REVS: Rank Editing in the Vocabulary Space for Unlearning Sensitive Information in Large Language Models},
        author={Ashuach, Tomer and Tutek, Martin and Belinkov, Yonatan},
        journal={arXiv preprint arXiv:2406.09325},
        year={2024}
      }</code></pre>
    </div>
</section>

<!--End BibTex citation -->
  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
      <p>
        This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.  
      </p>

    </div>
  </div>
</div>
  </div>
</footer>

<!-- Default Statcounter code for REVS Web https://tomertech.github.io/REVS-Web/
-->
<script type="text/javascript">
  var sc_project=13007732; 
  var sc_invisible=1; 
  var sc_security="bd519292"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="web stats"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/13007732/0/bd519292/1/" alt="web stats"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

  </body>
  </html>